{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid requirement: '/datasets/home/34/234/cs291gbl/fastText/. --user'\n",
      "It looks like a path. Does it exist ?\n",
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip.main(['install','/datasets/home/34/234/cs291gbl/fastText/. --user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pip\n",
    "import torch\n",
    "from torch import nn\n",
    "import argparse\n",
    "from torch.autograd import Variable\n",
    "import io\n",
    "from scipy import linalg\n",
    "# import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_embeddings(path, full_vocab=False):\n",
    "    \"\"\"\n",
    "    Reload pretrained embeddings from a text file.\n",
    "    \"\"\"\n",
    "    word2id = {}\n",
    "    vectors = []\n",
    "    id2word={}\n",
    "\n",
    "\n",
    "    emb_path = path\n",
    "    emb_dim_file = 300\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:\n",
    "                split = line.split()\n",
    "                assert len(split) == 2\n",
    "                assert emb_dim_file == int(split[1])\n",
    "            else:\n",
    "                word, vect = line.rstrip().split(' ', 1)\n",
    "                if not full_vocab:\n",
    "                    word = word.lower()\n",
    "                vect = np.fromstring(vect, sep=' ')\n",
    "                if np.linalg.norm(vect) == 0:  # avoid to have null embeddings\n",
    "                    vect[0] = 0.01\n",
    "\n",
    "                assert vect.shape == (emb_dim_file,), i\n",
    "                word2id[word] = len(word2id)\n",
    "                id2word[len(word2id)]=word\n",
    "                vectors.append(vect[None])\n",
    "            if len(word2id) >= 200000 and not full_vocab:\n",
    "                break\n",
    "\n",
    "    assert len(word2id) == len(vectors)\n",
    "    print(\"Loaded %i pre-trained word embeddings.\" % len(vectors))\n",
    "\n",
    "\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.concatenate(vectors, 0)\n",
    "    embeddings = torch.from_numpy(embeddings).float()\n",
    "    embeddings = embeddings\n",
    "    assert embeddings.size() == (200000, 300)\n",
    "    return embeddings,word2id,id2word\n",
    "\n",
    "#for loading fastText\n",
    "def load_fasttext_model(path):\n",
    "    \"\"\"\n",
    "    Load a binarized fastText model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import fastText\n",
    "    except ImportError:\n",
    "        raise Exception(\"Unable to import fastText. Please install fastText for Python: \"\n",
    "                        \"https://github.com/facebookresearch/fastText\")\n",
    "    return fastText.load_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 pre-trained word embeddings.\n",
      "Loaded 200000 pre-trained word embeddings.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# english=load_fasttext_model(\"data/wiki.en.bin\")\n",
    "# spanish=load_fasttext_model(\"data/wiki.es.bin\")\n",
    "src_embeddings,src_word2id,src_id2word=read_txt_embeddings(\"data/wiki.en.vec\")\n",
    "tgt_embeddings,tgt_word2id,tgt_id2word=read_txt_embeddings(\"data/wiki.es.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch2(X,M):\n",
    "    N = X.size()[0]\n",
    "    N=200000\n",
    "    batch_indices = torch.LongTensor( np.random.randint(0,N,size=M) )\n",
    "    batch_xs = torch.index_select(X,0,batch_indices)\n",
    "    #batch_ys = torch.index_select(Y,0,batch_indices)\n",
    "    return Variable(batch_xs, requires_grad=False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, dis_hid_dim,dis_dropout,dis_input_dropout):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.emb_dim = 300\n",
    "        self.dis_layers = 3\n",
    "        self.dis_hid_dim = dis_hid_dim\n",
    "        self.dis_dropout = dis_dropout\n",
    "        self.dis_input_dropout = dis_input_dropout\n",
    "\n",
    "        layers = [nn.Dropout(self.dis_input_dropout)]\n",
    "        for i in range(self.dis_layers + 1):\n",
    "            input_dim = self.emb_dim if i == 0 else self.dis_hid_dim\n",
    "            output_dim = 1 if i == self.dis_layers else self.dis_hid_dim\n",
    "            layers.append(nn.Linear(input_dim, output_dim))\n",
    "            if i < self.dis_layers:\n",
    "                layers.append(nn.LeakyReLU(0.2))\n",
    "                layers.append(nn.Dropout(self.dis_dropout))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == 2 and x.size(1) == self.emb_dim\n",
    "        return self.layers(x).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Orthogonal(mapping,beta):\n",
    "    W=mapping.weight.data\n",
    "    mapping.weight.data.copy_((1 + beta) * W - beta * W.mm(W.transpose(0, 1).mm(W)))\n",
    "#     print(mapping.weight.data.transpose(0, 1).mm(W))\n",
    "    return mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (layers): Sequential(\n",
       "    (0): Dropout(p=0.1)\n",
       "    (1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "    (2): LeakyReLU(0.2)\n",
       "    (3): Dropout(p=0.3)\n",
       "    (4): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (5): LeakyReLU(0.2)\n",
       "    (6): Dropout(p=0.3)\n",
       "    (7): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (8): LeakyReLU(0.2)\n",
       "    (9): Dropout(p=0.3)\n",
       "    (10): Linear(in_features=2048, out_features=1, bias=True)\n",
       "    (11): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set up linear mapping,\n",
    "mapping = nn.Linear(300, 300, bias=False)\n",
    "mapping.weight.data.copy_(torch.eye(300))\n",
    "modelDisc= Discriminator(2048,0.3,0.1)\n",
    "mapping.cuda()\n",
    "modelDisc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_avg_dist(emb, query, knn=10):\n",
    "    \"\"\"\n",
    "    Compute the average distance of the knn nearest neighbors\n",
    "    \"\"\"\n",
    "    bs = 1024\n",
    "    all_distances = []\n",
    "    emb = emb.transpose(0, 1).contiguous()\n",
    "    for i in range(0, query.shape[0], bs):\n",
    "        distances = query[i:i + bs].mm(emb)\n",
    "        best_distances, _ = distances.topk(knn, dim=1, largest=True, sorted=True)\n",
    "        all_distances.append(best_distances.mean(1).cpu())\n",
    "    all_distances = torch.cat(all_distances)\n",
    "    return all_distances.numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(emb1,emb2):\n",
    "    bs = 128\n",
    "\n",
    "    all_scores = []\n",
    "    all_targets = []\n",
    "\n",
    "    # number of source words to consider\n",
    "    n_src = emb1.size(0)\n",
    "    n_src =15000\n",
    "    knn = 10\n",
    "\n",
    "        # average distances to k nearest neighbors\n",
    "    average_dist1 = torch.from_numpy(get_nn_avg_dist(emb2, emb1, knn))\n",
    "    average_dist2 = torch.from_numpy(get_nn_avg_dist(emb1, emb2, knn))\n",
    "    average_dist1 = average_dist1.type_as(emb1)\n",
    "    average_dist2 = average_dist2.type_as(emb2)\n",
    "\n",
    "    # for every source word\n",
    "    for i in range(0, n_src, bs):\n",
    "\n",
    "        # compute target words scores\n",
    "        scores = emb2.mm(emb1[i:min(n_src, i + bs)].transpose(0, 1)).transpose(0, 1)\n",
    "        scores.mul_(2)\n",
    "        scores.sub_(average_dist1[i:min(n_src, i + bs)][:, None] + average_dist2[None, :])\n",
    "        best_scores, best_targets = scores.topk(2, dim=1, largest=True, sorted=True)\n",
    "\n",
    "        # update scores / potential targets\n",
    "        all_scores.append(best_scores.cpu())\n",
    "        all_targets.append(best_targets.cpu())\n",
    "\n",
    "    all_scores = torch.cat(all_scores, 0)\n",
    "    all_targets = torch.cat(all_targets, 0)\n",
    "\n",
    "    all_pairs = torch.cat([torch.arange(0, all_targets.size(0)).long().unsqueeze(1),all_targets[:, 0].unsqueeze(1)], 1)\n",
    "    assert all_scores.size() == all_pairs.size() == (n_src, 2)\n",
    "\n",
    "    # sort pairs by score confidence\n",
    "    diff = all_scores[:, 0] - all_scores[:, 1]\n",
    "    reordered = diff.sort(0, descending=True)[1]\n",
    "    all_scores = all_scores[reordered]\n",
    "    all_pairs = all_pairs[reordered]\n",
    "    \n",
    "    \n",
    "    selected = all_pairs.max(1)[0] <= 15000\n",
    "    mask = selected.unsqueeze(1).expand_as(all_scores).clone()\n",
    "    all_scores = all_scores.masked_select(mask).view(-1, 2)\n",
    "    all_pairs = all_pairs.masked_select(mask).view(-1, 2)\n",
    "    return all_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dictionary(src_embedding,tgt_embedding):\n",
    "    src2tgt = get_candidates(src_embedding,tgt_embedding)\n",
    "    tgt2src = get_candidates(tgt_embedding,src_embedding)\n",
    "    \n",
    "    s2t_candidates = set([(a, b) for a, b in src2tgt])\n",
    "    t2s_candidates = set([(a, b) for a, b in tgt2src])\n",
    "#     print(s2t_candidates)\n",
    "    final_pairs = s2t_candidates & t2s_candidates\n",
    "    if len(final_pairs) == 0:\n",
    "        print(\"Dictionary does not exist\")\n",
    "        return None\n",
    "    dico = torch.LongTensor(list([[a, b] for (a, b) in final_pairs]))\n",
    "\n",
    "    print('New train dictionary of %i pairs.' % dico.size(0))\n",
    "    return dico.cuda()\n",
    "    \n",
    "        \n",
    "# def build_dictionary(src_emb, tgt_emb, _params, s2t_candidates, t2s_candidates):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_mean_cosine(src_emb, tgt_emb):\n",
    "        \"\"\"\n",
    "        Mean-cosine model selection criterion.\n",
    "        \"\"\"\n",
    "        \n",
    "        # get normalized embeddings\n",
    "        src_emb = mapping(Variable(src_emb, requires_grad=False).cuda())\n",
    "        src_emb=torch.FloatTensor(src_emb.cpu().data.numpy()).cuda()\n",
    "        tgt_emb = tgt_emb.cuda()\n",
    "        src_emb = src_emb / src_emb.norm(2, 1, keepdim=True).expand_as(src_emb)\n",
    "        tgt_emb = tgt_emb / tgt_emb.norm(2, 1, keepdim=True).expand_as(tgt_emb)\n",
    "        \n",
    "\n",
    "        dico = create_dictionary(src_emb, tgt_emb)\n",
    "        dico_max_size = 10000\n",
    "\n",
    "      \n",
    "        if dico is None:\n",
    "            mean_cosine = -1e9\n",
    "        else:\n",
    "            mean_cosine = (src_emb[dico[:dico_max_size, 0]] * tgt_emb[dico[:dico_max_size, 1]]).sum(1).mean()\n",
    "        print(\"Mean cosine\", mean_cosine)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "linoptimizer = torch.optim.SGD(mapping.parameters(), lr=0.1)\n",
    "discoptimizer= torch.optim.SGD(modelDisc.parameters(), lr=0.1)\n",
    "\n",
    "loss_fn=torch.nn.BCELoss()\n",
    "\n",
    "def get_xy():\n",
    "    \n",
    "    #get minibatch of spanish words\n",
    "    spanish_batch=get_batch2(tgt_embeddings,batch_size)\n",
    "    #getminibatch of english words\n",
    "    english_batch=get_batch2(src_embeddings,batch_size)\n",
    "\n",
    "    #generate fake spanish embeddings\n",
    "    s_fake= mapping(english_batch)\n",
    "\n",
    "    #stack real and fake\n",
    "    x = torch.cat([spanish_batch, s_fake], 0)\n",
    "    y = torch.FloatTensor(2 * batch_size).zero_()\n",
    "    \n",
    "    y[batch_size:] = 0.05\n",
    "    y[:batch_size]=1-0.05\n",
    "    y=Variable(y,requires_grad=False).cuda()\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary does not exist\n",
      "('Mean cosine', -1000000000.0)\n",
      "Dictionary does not exist\n",
      "('Mean cosine', -1000000000.0)\n",
      "Dictionary does not exist\n",
      "('Mean cosine', -1000000000.0)\n",
      "New train dictionary of 2 pairs.\n",
      "('Mean cosine', 0.638550877571106)\n",
      "New train dictionary of 7 pairs.\n",
      "('Mean cosine', 0.601514995098114)\n",
      "New train dictionary of 4 pairs.\n",
      "('Mean cosine', 0.6637828946113586)\n"
     ]
    }
   ],
   "source": [
    "epoch=100000\n",
    "batch_size=32\n",
    "# dog=Variable(src_embeddings[src_word2id['dog']]).cuda()\n",
    "\n",
    "# perro=Variable(tgt_embeddings[tgt_word2id['perro']]).cuda()\n",
    "for _ in range(epoch):\n",
    "      \n",
    "    \n",
    "    modelDisc.train()\n",
    "    mapping.eval()\n",
    "    for __ in range(5):\n",
    "        \n",
    "        x,y=get_xy()\n",
    "        y_pred = modelDisc(x)\n",
    "        # Compute and print loss.\n",
    "        \n",
    "\n",
    "        loss= loss_fn(y_pred, y)\n",
    "\n",
    "        discoptimizer.zero_grad()\n",
    "       \n",
    "        # Backward pass: compute gradient of the loss with respect to model\n",
    "        # parameters\n",
    "        loss.backward()\n",
    "#         print(loss.data)\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        discoptimizer.step()\n",
    "        \n",
    "        \n",
    "    for __ in range(3):\n",
    "        modelDisc.eval()\n",
    "        mapping.train()\n",
    "        x,y=get_xy()\n",
    "        y=1-y\n",
    "        y_pred = modelDisc(x)\n",
    "        loss2= loss_fn(y_pred, y)\n",
    "        linoptimizer.zero_grad()\n",
    "        loss2.backward()\n",
    "        linoptimizer.step()\n",
    "        \n",
    "        if _%1:\n",
    "            mapping=Orthogonal(mapping,0.001)\n",
    "        \n",
    "    if _%10000==0:\n",
    "        dist_mean_cosine(src_embeddings,tgt_embeddings)\n",
    "#         cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    #     print(type(perro),type(mapping(dog).cpu()))\n",
    "#     print(cos(perro.cpu(),mapping(dog).cpu()).data)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # spanish_embeddings\n",
    "# tgt_emb2=tgt_embeddings.cpu().numpy()\n",
    "# # print(tgt_emd.shape)\n",
    "\n",
    "\n",
    "def get_nn(word_emb, tgt_emb, tgt_id2word, K=5):\n",
    "  \n",
    "    scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
    "    k_best = scores.argsort()[-K:][::-1]\n",
    "    for i, idx in enumerate(k_best):\n",
    "        print('%.4f - %s' % (scores[idx], tgt_id2word[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_map(words,language):\n",
    "#     id2word={}\n",
    "#     for word in words:\n",
    "#         id=language.get_word_id(word)\n",
    "#         id2word[id]=word\n",
    "#     return id2word\n",
    "# spanish_map=create_map(Swords,spanish)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cat_embedding=mapping(Variable(src_embeddings[src_word2id['same']],requires_grad=False).cuda()).cpu().data\n",
    "# word_emd=cat_embedding.numpy()\n",
    "# get_nn(word_emd,tgt_emb,tgt_id2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_embedding=tgt_embeddings[tgt_word2id['misma']]\n",
    "# word_emd=cat_embedding.numpy()\n",
    "# get_nn(word_emd,fake_src_embeddings.cpu().numpy(),src_id2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_src_embeddings=(mapping(Variable(src_embeddings, requires_grad=False).cuda())).cuda()\n",
    "tgt_embeddings=(tgt_embeddings).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_src_embeddings=torch.FloatTensor(fake_src_embeddings.cpu().data.numpy()).cuda()\n",
    "# type(fake_src_embeddings)\n",
    "# src_embedding=src_embeddings.cpu().detach().numpy()\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=2, whiten=True)  # TSNE(n_components=2, n_iter=3000, verbose=2)\n",
    "# pca.fit(np.vstack([src_embedding, spanish_embeddings]))\n",
    "# print('Variance explained: %.2f' % pca.explained_variance_ratio_.sum())\n",
    "src_emb=fake_src_embeddings/fake_src_embeddings.norm(2, 1, keepdim=True).expand_as(fake_src_embeddings)\n",
    "tgt_emb = tgt_embeddings / tgt_embeddings.norm(2, 1, keepdim=True).expand_as(tgt_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# def plot_similar_word(src_words, src, src_emb, tgt_words, tgt, tgt_emb, pca):\n",
    "\n",
    "#     Y = []\n",
    "#     word_labels = []\n",
    "#     for sw in src_words:\n",
    "#         Y.append(src_emb[src.get_word_id(sw)])\n",
    "#         word_labels.append(sw)\n",
    "#     for tw in tgt_words:\n",
    "#         Y.append(tgt_emb[tgt.get_word_id(tw)])\n",
    "#         word_labels.append(tw)\n",
    "\n",
    "#     # find tsne coords for 2 dimensions\n",
    "#     Y = pca.transform(Y)\n",
    "#     x_coords = Y[:, 0]\n",
    "#     y_coords = Y[:, 1]\n",
    "\n",
    "#     # display scatter plot\n",
    "#     plt.figure(figsize=(10, 8), dpi=80)\n",
    "#     plt.scatter(x_coords, y_coords, marker='x')\n",
    "\n",
    "#     for k, (label, x, y) in enumerate(zip(word_labels, x_coords, y_coords)):\n",
    "#         color = 'blue' if k < len(src_words) else 'red'  # src words in blue / tgt words in red\n",
    "#         plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points', fontsize=19,\n",
    "#                      color=color, weight='bold')\n",
    "\n",
    "#     plt.xlim(x_coords.min() - 0.2, x_coords.max() + 0.2)\n",
    "#     plt.ylim(y_coords.min() - 0.2, y_coords.max() + 0.2)\n",
    "#     plt.title('Visualization of the multilingual word embedding space')\n",
    "\n",
    "#     plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_words = ['dog', 'the', 'the', 'tennis', 'cat', 'conference']\n",
    "# tgt_words = ['perro', 'el', 'la', u'tenis',  'gato', 'conferencia']\n",
    "\n",
    "# plot_similar_word(src_words,english,english_embeddings.cpu().detach().numpy(),tgt_words,spanish,spanish_embeddings.cpu().detach().numpy(),pca)\n",
    "# # english.get_word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-d91d40c3b15c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdico\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(type(src_embedding))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-9707d71e3e8d>\u001b[0m in \u001b[0;36mcreate_dictionary\u001b[0;34m(src_embedding, tgt_embedding)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_embedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msrc2tgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_embedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtgt2src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_embedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0ms2t_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msrc2tgt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-dd170a68904d>\u001b[0m in \u001b[0;36mget_candidates\u001b[0;34m(emb1, emb2)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# update scores / potential targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mall_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mall_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;34mr\"\"\"Returns a CPU copy of this tensor if it's not already on the CPU\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0m__new__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/torch/_utils.pyc\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dico=create_dictionary(src_emb,tgt_emb)\n",
    "# print(type(src_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    " def procrustes(src_emb,tgt_emb):\n",
    "        \"\"\"\n",
    "        Find the best orthogonal matrix mapping using the Orthogonal Procrustes problem\n",
    "        https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem\n",
    "        \"\"\"\n",
    "        A = src_emb[dico[:, 0]]\n",
    "        B = tgt_emb[dico[:, 1]]\n",
    "        W = mapping.weight.data\n",
    "        M = B.transpose(0, 1).mm(A).cpu().numpy()\n",
    "        U, S, V_t = linalg.svd(M, full_matrices=True)\n",
    "        mapping.weight.data.copy_(torch.from_numpy(U.dot(V_t)).type_as(W))\n",
    "#         return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    procrustes(src_emb,tgt_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-7.8339e-01  2.0606e-01 -1.1714e-01  ...  -1.0379e-02  6.0843e-03  2.0496e-02\n",
       "-2.4279e-01  1.0654e-01 -3.6599e-03  ...  -2.8994e-02 -1.4005e-01 -2.3610e-02\n",
       " 1.0801e-01  4.1441e-02  1.7987e-01  ...  -5.9630e-03 -1.2171e-02  1.8207e-02\n",
       "                ...                   ⋱                   ...                \n",
       " 3.1065e-02  1.6978e-02  2.2879e-02  ...   1.5515e-04 -6.9401e-02 -2.6943e-02\n",
       "-2.1687e-02 -3.4335e-02  4.5958e-02  ...   8.4566e-03  1.0589e-02 -6.1813e-02\n",
       "-1.6965e-03 -1.1995e-02 -1.7960e-02  ...   1.3065e-02  1.6446e-02 -5.5869e-02\n",
       "[torch.cuda.FloatTensor of size 300x300 (GPU 0)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_src_embeddings2=(mapping(Variable(src_embeddings, requires_grad=False).cuda())).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1912 - fake\n",
      "0.1863 - dormal\n",
      "0.1809 - christina\n",
      "0.1776 - lung\n",
      "0.1758 - alternativo\n"
     ]
    }
   ],
   "source": [
    "cat_embedding=mapping(Variable(src_embeddings[src_word2id['the']],requires_grad=False).cuda()).cpu().data\n",
    "word_emd=cat_embedding.numpy()\n",
    "get_nn(word_emd,tgt_embeddings.cpu().numpy(),tgt_id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
